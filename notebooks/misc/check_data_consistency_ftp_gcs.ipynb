{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import google.cloud.client\n",
    "\n",
    "from google.cloud import storage\n",
    "dev_storage_client = storage.Client()\n",
    "\n",
    "import ftplib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overall \"environment\" variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wget2_flags = '-b -c -nv -nH --report-speed=bytes --progress=dot'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ftp_site = 'ftp.1000genomes.ebi.ac.uk'\n",
    "\n",
    "ftp_data_root = '/vol1/ftp/data_collections/HGSVC2/working'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gcs_bucket_name = \"broad-dsde-methods-long-reads\"\n",
    "gcs_root_prefix = 'datasets/HGSVC2'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gcsfuse_local_dir = 'gs'\n",
    "gcs_leading_prefix = f'gs://{gcs_bucket_name}/{gcs_root_prefix}'.rstrip('/') + '/'\n",
    "gcsfuse_local_leading_prefix = f'{gcsfuse_local_dir}/{gcs_root_prefix}'.rstrip('/') + '/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gcs_dirs = ['20190925_PUR_PacBio_HiFi',\n",
    "            '20191005_YRI_PacBio_NA19240_HiFi',\n",
    "            '20191031_CHS_PacBio_HG00512_HiFi',\n",
    "            '20191031_CHS_PacBio_HG00513_HiFi',\n",
    "            '20191205_YRI_PacBio_NA19238_HIFI',\n",
    "            '20191205_YRI_PacBio_NA19239_HIFI',\n",
    "            '20200108_PacBio_CLR_JAX',\n",
    "            '20200203_PacBio_CLR_EEE',\n",
    "            '20200212_PacBio_CLR_Devine',\n",
    "            '20200722_PUR_PacBio_HG00732_HiFi',\n",
    "            '20200731_CHS_PacBio_HG00514_CLR_reseq',\n",
    "            '20200731_CHS_PacBio_HG00514_HiFi_reseq',\n",
    "            '20210509_UW_HiFi',\n",
    "            '2021_PacBio_HIFI_JAX'\n",
    "            ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "known_data_directories_on_ftp = ['20190925_PUR_PacBio_HiFi/',\n",
    "                                 '20191005_YRI_PacBio_NA19240_HiFi/',\n",
    "                                 '20191031_CHS_PacBio_HG00512_HiFi/',\n",
    "                                 '20191031_CHS_PacBio_HG00513_HiFi/',\n",
    "                                 '20191205_YRI_PacBio_NA19238_HIFI/',\n",
    "                                 '20191205_YRI_PacBio_NA19239_HIFI/',\n",
    "                                 '20200108_PacBio_CLR_JAX/',\n",
    "                                 '20200203_PacBio_CLR_EEE/',\n",
    "                                 '20200212_PacBio_CLR_Devine/',\n",
    "                                 '20200722_PUR_PacBio_HG00732_HiFi/',\n",
    "                                 '20200731_CHS_PacBio_HG00514_CLR_reseq/',\n",
    "                                 '20200731_CHS_PacBio_HG00514_HiFi_reseq/',\n",
    "                                 '20210509_UW_HiFi/',\n",
    "                                 '20210822_UW_EEE_ONT_UL/',\n",
    "                                 '20210920_ONT_Rebasecalled/',\n",
    "                                 '20211013_ONT_Rebasecalled/',\n",
    "                                 '2021_ONT_UltraLong_JAX/',\n",
    "                                 '2021_PacBio_HIFI_JAX/'\n",
    "                                 ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dir = gcs_dirs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities for exploring FTP site"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def traverse_ftp_data_dir(configured_ftp: ftplib.FTP, working_dir: str, depth: int=0):\n",
    "    \"\"\"\n",
    "    Return a recursive listing of an ftp server contents, under the current directory.\n",
    "\n",
    "    :param configured_ftp: ftplib.FTP object with the intended directory to explore\n",
    "    :param working_dir: current directory being explored\n",
    "    :param depth: depth into the origin dir\n",
    "    :return: a recursive dictionary, where\n",
    "                each key is the name of an entity (a file or a sub-directory), and\n",
    "                its value is its size in bytes in case of a file, or\n",
    "                its contents (a dict) in case of a sub-directory.\n",
    "    \"\"\"\n",
    "\n",
    "    if depth > 10:\n",
    "        raise ValueError(\"Tree is too deep, I give up.\")\n",
    "    level = {}\n",
    "    contents = []\n",
    "    configured_ftp.cwd(working_dir)\n",
    "    configured_ftp.retrlines('LIST', callback=lambda s: contents.append(s.split()))\n",
    "    for entry in contents:\n",
    "        mode = entry[0]\n",
    "        name = entry[-1]\n",
    "        if mode.startswith('d'):  # it's a sub-dir (note this is a hack, check this out: https://bit.ly/31zQWLS)\n",
    "            level[name] = traverse_ftp_data_dir(configured_ftp, name, depth+1)\n",
    "            configured_ftp.cwd('..')\n",
    "        else:  # it's a regular file\n",
    "            level[name] = configured_ftp.size(name)\n",
    "    return level\n",
    "\n",
    "def flatten_ftp_dir_tree(nested_dict_tree: dict,\n",
    "                         configured_ftp: ftplib.FTP,\n",
    "                         data_dir: str):\n",
    "    \"\"\"\n",
    "    Given the nested dir returned by `traverse_ftp_data_dir`, flatten it to a 1-D\n",
    "    dict where the key is the path to a file, and value is its size in bytes.\n",
    "\n",
    "    :param nested_dict_tree: the structure returned by `traverse_ftp_data_dir`\n",
    "    :param configured_ftp: a ftplib.FTP object where the nested dir was generated on\n",
    "    :param data_dir: the origin dir where `traverse_ftp_data_dir` was run on\n",
    "    :return: A list where the entries are absolute paths to each individual file\n",
    "    \"\"\"\n",
    "    flat_file_list = dict()\n",
    "    for k, v in nested_dict_tree.items():\n",
    "        if isinstance(v, dict):\n",
    "            child = flatten_ftp_dir_tree(nested_dict_tree.get(k), configured_ftp = configured_ftp,\n",
    "                                         data_dir = f'{data_dir}/{k}')\n",
    "            flat_file_list.update(child)\n",
    "        else:\n",
    "            name = f'{configured_ftp.host}{ftp_data_root}/{data_dir}/{k}'\n",
    "            flat_file_list[name] = v\n",
    "    return flat_file_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with (ftplib.FTP(ftp_site)) as ftp_socket:\n",
    "    ftp_socket.login()\n",
    "    to_explore = ftp_data_root + '/' + test_dir\n",
    "    print(f\"Exploring {to_explore}\")\n",
    "    test_tree = traverse_ftp_data_dir(ftp_socket, working_dir=to_explore)\n",
    "    test_flat = flatten_ftp_dir_tree(test_tree, ftp_socket, test_dir)\n",
    "    ftp_socket.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pp.pprint(test_tree)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_flat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilities for exploring GCS buckets and \"folders\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def list_gcs_subdirectories(client: google.cloud.client.Client, bucket_name: str, prefix: str):\n",
    "    \"\"\"\n",
    "    List sub-directories under 'gs://[bucket_name]/[prefix]/'\n",
    "    Based on https://github.com/googleapis/google-cloud-python/issues/920#issuecomment-653823847\n",
    "    :param client: GCS client\n",
    "    :param bucket_name: name of the bucket\n",
    "    :param prefix: the prefix, or the full path to the directory to list into. Note: don't include leading and ending slash.\n",
    "    :return: A list of sub-directories under the prefix 'directory' in the provided bucket\n",
    "    \"\"\"\n",
    "\n",
    "    local_prefix = prefix if prefix.endswith('/') else prefix + '/'\n",
    "    iterator = client.list_blobs(bucket_name, prefix=local_prefix, delimiter='/')\n",
    "    prefixes = set()\n",
    "    for page in iterator.pages:\n",
    "        prefixes.update(page.prefixes)\n",
    "\n",
    "    return list([s.split('/')[-2] for s in prefixes])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def list_file_base_names(client: google.cloud.client.Client, bucket_name: str, prefix: str):\n",
    "    \"\"\"\n",
    "    Give base name of files under 'gs://[bucket_name]/[prefix]/'\n",
    "    :param client: GCS client\n",
    "    :param bucket_name: name of the bucket\n",
    "    :param prefix: the prefix, or the full path to the directory to list into. Note: don't include leading and ending slash.\n",
    "    :return: base name of files under the prefix 'directory' in the provided bucket\n",
    "    \"\"\"\n",
    "\n",
    "    full_path = [b.name for b in client.list_blobs(bucket_or_name=bucket_name, prefix=prefix)]\n",
    "    return [re.sub(f'^{prefix}', '', path).lstrip('/') for path in full_path]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GcsPath:\n",
    "    \"\"\"\n",
    "    Modeling after GCS storage object, offering simplistic way of\n",
    "        * checking if the paths exists, and if exists,\n",
    "        * represents a file or\n",
    "        * emulates a 'directory'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gs_path: str):\n",
    "\n",
    "        if not gs_path.startswith(\"gs://\"):\n",
    "            raise ValueError(f\"Provided gs path isn't valid: {gs_path}\")\n",
    "\n",
    "        arr = re.sub(\"^gs://\", '', gs_path).split('/')\n",
    "        self.bucket = arr[0]\n",
    "        self.prefix = '/'.join(arr[1:-1])\n",
    "        self.file = arr[-1]\n",
    "\n",
    "    def exists(self, client: storage.client.Client) -> bool:\n",
    "        return self.is_file(client=client) or self.is_emulate_dir(client=client)\n",
    "\n",
    "    def is_file(self, client: storage.client.Client) -> bool:\n",
    "        return storage.Blob(bucket=client.bucket(self.bucket), name=f'{self.prefix}/{self.file}').exists(client)\n",
    "\n",
    "    def is_emulate_dir(self, client: storage.client.Client) -> bool:\n",
    "        if self.is_file(client=client):\n",
    "            return False\n",
    "        return any(True for _ in client.list_blobs(client.bucket(self.bucket), prefix=f'{self.prefix}/{self.file}'))\n",
    "\n",
    "    def size(self, client: storage.client.Client) -> int:\n",
    "        blob = storage.Blob(bucket=client.bucket(self.bucket), name=f'{self.prefix}/{self.file}')\n",
    "        if blob.exists(client=client):\n",
    "            blob.reload()\n",
    "            return blob.size\n",
    "        else:\n",
    "            return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# traverse each dir already on GCS, and check which ones are missing compared to its mirroring folder on FTP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collect_files_to_transfer(gcs_dir_to_check: str, gcs_client: google.cloud.storage.Client,\n",
    "                              ftp_server: str, ftp_data_root_dir: str,\n",
    "                              gcs_bucket_name: str, gcs_root_prefix: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a 'folder' on GCS to check data consistency (against a FTP folder), collect which files\n",
    "       1) have been successfully downloaded,\n",
    "       2) have been only partially downloaded,\n",
    "       3) have not been downloaded at all\n",
    "    The way this is checked is by using file sizes (a compromise compared to MD5 check).\n",
    "    :param gcs_dir_to_check: 'folder' on GCS to check\n",
    "    :param gcs_client: GCS python API client\n",
    "    :param ftp_server: FTP server address\n",
    "    :param ftp_data_root_dir: root dir holding data, which is supposed to be mirrored by gcs_dir_to_check\n",
    "    :param gcs_bucket_name: GCS bucket hosting the dir\n",
    "    :param gcs_root_prefix: root prefix, i.e. 'parent folder' of gcs_dir_to_check, under the hosting bucket\n",
    "    :return: a tuple-3 of (fully downloaded, incompletely downloaded, not yet downloaded) files\n",
    "    \"\"\"\n",
    "\n",
    "    finished = list()\n",
    "    incomplete_download = dict()\n",
    "    not_yet_on_gcs = dict()\n",
    "    with (ftplib.FTP(ftp_server)) as ftp_socket:\n",
    "        ftp_socket.login()\n",
    "        ftp_socket.cwd(ftp_data_root_dir)\n",
    "\n",
    "        print(f\"Traversing test data dir {ftp_data_root_dir}/{gcs_dir_to_check} on FTP server {ftp_server} ...\")\n",
    "        ftp_dir_tree = traverse_ftp_data_dir(ftp_socket, working_dir=gcs_dir_to_check)\n",
    "        ftp_socket.quit()\n",
    "        ftp_files_flat = flatten_ftp_dir_tree(ftp_dir_tree, ftp_socket, gcs_dir_to_check)\n",
    "        print(f\"Found {len(ftp_files_flat)} files under {gcs_dir_to_check}\")\n",
    "\n",
    "    print(\"Matching FTP files with GCS files...\")\n",
    "\n",
    "    for ff, sz in ftp_files_flat.items():\n",
    "        gs_path = re.sub(f'^{ftp_server}{ftp_data_root}', f'gs://{gcs_bucket_name}/{gcs_root_prefix}', ff)\n",
    "        to_check = GcsPath(gs_path)\n",
    "        if to_check.exists(client=gcs_client):\n",
    "            gcs_sz = to_check.size(client=gcs_client)\n",
    "            if gcs_sz is None:\n",
    "                raise ValueError(f\"{gs_path} exist on GCS but doesn't have a size...\")\n",
    "            if 0 == gcs_sz:\n",
    "                not_yet_on_gcs[ff] = gs_path\n",
    "            elif sz > gcs_sz:\n",
    "                incomplete_download[ff] = {'full': sz, 'has': gcs_sz, 'gcs_path': gs_path}\n",
    "            else:\n",
    "                finished.append(ff)\n",
    "        else:\n",
    "            not_yet_on_gcs[ff] = gs_path\n",
    "\n",
    "    return finished, incomplete_download, not_yet_on_gcs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ok, corrupt, jobs = collect_files_to_transfer(gcs_dir_to_check=test_dir, gcs_client=dev_storage_client,\n",
    "                                              ftp_server=ftp_site, ftp_data_root_dir=ftp_data_root,\n",
    "                                              gcs_bucket_name=gcs_bucket_name, gcs_root_prefix=gcs_root_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrupt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test one dir where there's sub-dirs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dir = '20200108_PacBio_CLR_JAX'\n",
    "ok, corrupt, jobs = collect_files_to_transfer(gcs_dir_to_check=test_dir, gcs_client=dev_storage_client,\n",
    "                                              ftp_server=ftp_site, ftp_data_root_dir=ftp_data_root,\n",
    "                                              gcs_bucket_name=gcs_bucket_name, gcs_root_prefix=gcs_root_prefix)\n",
    "corrupt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now real check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_files_per_dataset = dict()\n",
    "for d in gcs_dirs:\n",
    "    print(\"====================================================================================================\")\n",
    "    ok, corrupt, fresh = collect_files_to_transfer(gcs_dir_to_check=d, gcs_client=dev_storage_client,\n",
    "                                                   ftp_server=ftp_site, ftp_data_root_dir=ftp_data_root,\n",
    "                                                   gcs_bucket_name=gcs_bucket_name, gcs_root_prefix=gcs_root_prefix)\n",
    "    missing_files_per_dataset[d] = {'corrupt': corrupt, 'fresh': fresh}\n",
    "    print(\"====================================================================================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def keep_file(file_full_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Models a file filter, heavily project-dependent.\n",
    "    \"\"\"\n",
    "\n",
    "    keep = not file_full_name.endswith('.fastq.gz')\n",
    "\n",
    "    is_meant_for_ccs = any(pat in file_full_name for pat in ['ccs', 'CCS', 'HiFi', 'HIFI'])\n",
    "    is_pointing_to_non_ccs = any(pat in file_full_name for pat in ['subreads', 'scraps'])\n",
    "    if is_meant_for_ccs and is_pointing_to_non_ccs:\n",
    "        return False\n",
    "\n",
    "    return keep"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_missing_files = {k:v['fresh'] for k,v in missing_files_per_dataset.items()}\n",
    "all_missing_files = {k: {ik: iv for ik, iv in v.items() if keep_file(iv)} for k, v in all_missing_files.items()}\n",
    "all_missing_files = {k: v for k, v in all_missing_files.items() if v}\n",
    "print(f\"{sum(len(v) for _, v in all_missing_files.items())} files in total to download, afresh.\")\n",
    "pp.pprint(all_missing_files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_corrupt_files = {k: {ik: iv['gcs_path'] for ik, iv in v['corrupt'].items()} for k,v in missing_files_per_dataset.items()}\n",
    "all_corrupt_files = {k: {ik: iv for ik, iv in v.items() if keep_file(iv)} for k, v in all_corrupt_files.items()}\n",
    "all_corrupt_files = {k: v for k, v in all_corrupt_files.items() if v}\n",
    "print(f\"{sum(len(v) for _, v in all_corrupt_files.items())} files in total to download, again.\")\n",
    "pp.pprint(all_corrupt_files)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### print out file downloading commands"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 0 < len(all_missing_files):\n",
    "    for k,v in all_missing_files.items():\n",
    "        for ftp_path, gs_path in v.items():\n",
    "\n",
    "            local_relative_path = re.sub(gcs_leading_prefix, '', gs_path)\n",
    "            local_prefix = gcsfuse_local_leading_prefix + '/'.join(local_relative_path.split('/')[:-1]) + '/'\n",
    "            log_file = '__'.join(ftp_path.split('/')[6:]) + \".wget-log\"\n",
    "            cmd = f'wget2 {wget2_flags} \\\\\\n  -P {local_prefix} \\\\\\n  -o {log_file} \\\\\\n  {ftp_path}'\n",
    "            print(cmd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if 0 < len(all_corrupt_files):\n",
    "    # # remove corrupt files from cloud first:\n",
    "    # # !!!!!!!!!! MAKE SURE THEY ARE NOT IN THE PROCESS OF BEING DOWNLOADED !!!!!!!!!!\n",
    "    # for k, v in all_corrupt_files.items():\n",
    "    #     for _, gs_path in v.items():\n",
    "    #         cmd = f'gsutil rm {gs_path}'\n",
    "    #         os.system(cmd)\n",
    "    for k,v in all_corrupt_files.items():\n",
    "        for ftp_path, gs_path in v.items():\n",
    "\n",
    "            local_relative_path = re.sub(gcs_leading_prefix, '', gs_path)\n",
    "            local_prefix = gcsfuse_local_leading_prefix + '/'.join(local_relative_path.split('/')[:-1]) + '/'\n",
    "            log_file = '__'.join(ftp_path.split('/')[6:]) + \".wget-log\"\n",
    "            cmd = f'wget2 {wget2_flags} \\\\\\n  -P {local_prefix} \\\\\\n  -o {log_file} \\\\\\n  {ftp_path}'\n",
    "            print(cmd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}